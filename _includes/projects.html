<section class="bg-dark no-padding" id="Projects">
    <div class="container no-gutter">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">Selected Projects</h2>
                <hr class="primary">
            </div>
        </div>
        <div class="row ">
            <blockquote class="h4 text-left">
                <a href="project-auto_arrangement">Algorithm Arrangement via Search and Style Transfer</a>
            </blockquote>
            <div class="col-sm-4">
                <a href="project-auto_arrangement" class="portfolio-box">
                    <img src="img/portfolio/1.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Demo Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-sm-8">                
                <ul class="list-inline" >
                    <ul class='text-justify'>
                        <li><p> Accompaniment generation for lead melodies is an interesting topic in computer music. While conventional methods rely on seq2seq models, we propose a novel framework based on rule-based search strategies and style transfer in the disentangled latent space.  </p></li>

                        <li><p>This project is a joint application of <a href="https://arxiv.org/abs/1906.03626" class="text-white bg-dark">EC<sup>2</sup>-VAE</a>, <a href="https://arxiv.org/abs/2008.07122" class="text-white bg-dark">Poly-Disentanglement-VAE</a>, and <a href="https://arxiv.org/abs/2008.07142" class="text-white bg-dark">POP-909 Dataset</a>, three ISMIR works from <a href="http://www.musicxlab.com/#/" class="text-white bg-dark">Music X Lab</a>, NYU Shanghai.</p></li>

                        <li><p><a href="project-auto_arrangement" class="text-white bg-dark">Demo Page</a>, <a href="https://github.com/zhaojw1998/Accompaniment-Generation-via-Music-Representation-Learning" class="text-white bg-dark">Github Page</a></p></li>
                    </ul>
                </ul>
            </div>
        </div>

        <p></p>

        <div class="row ">
            <blockquote class="h4 text-left">
                <a href="project-adversarial_ec2vae">Adversarial Explicitly-Constrained Conditional VAE</a>
            </blockquote>
            <div class="col-sm-4">
                <a href="project-adversarial_ec2vae" class="portfolio-box">
                    <img src="img/portfolio/1.5.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Demo Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-sm-8">                
                <ul class="list-inline" >
                    <ul class='text-justify'>
                        <li><p> One of the key issues of music generation is to control the generation process. EC2-VAE is a chord controlled melody generation model which disentangles the melody into pitch and rhythm. However, the modelâ€™s latent pitch encoding is not a chord-invariant representation, and thus cannot induce the network to learn a chord-melody dependency. </p></li>

                        <li><p>This project introduces adversarial training to detach chord cues from pitch, and contributes a novel disentanglement methodology for controllable generation.</p></li>

                        <li><p><a href="project-adversarial_ec2vae" class="text-white bg-dark">Demo Page</a>, <a href="https://github.com/zhaojw1998/Adversarial-EC2-VAE--Melody-Generation-towards-Better-Chord-Control" class="text-white bg-dark">Github Page</a></p></li>
                    </ul>
                </ul>
            </div>
        </div>

        <p></p>
        
        <div class="row">
            <blockquote class="h4 text-left">
                <a href="project-dancing_robot">Real-Time Music-Driven Dance Generation for Humanoid Robot</a>
            </blockquote>
            <div class="col-sm-4">
                <a href="project-dancing_robot" class="portfolio-box">
                    <img src="img/portfolio/2.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Demo Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-sm-8">                
                <ul class="list-inline">
                    <ul class='text-justify'>
                        <li><p>This project aims to implement a real-time music-driven robotic dancer, and we propose to generate improvisational dance sequence with Unit Selection Methodology based on two criteria: semantic relevance of dance units, and unit transition stability of the robot.</p></li>

                        <li><p>To render real-time performance, we apply Madmom for beat tracking and synchronize our dance to the music through a feedback loop with PID control.</p></li>

                        <li><p><a href="project-auto_arrangement" class="text-white bg-dark">Demo Page</a>, <a href="https://github.com/zhaojw1998/Real-Time-Music-Driven-Dancing-Robot" class="text-white bg-dark">Github Page</a></p></li>
                    </ul>
                </ul>
            </div>
        </div>

        <p></p>

        <div class="row">
            <blockquote class="h4 text-left">
                <a href="https://github.com/zhaojw1998/SoundtrackClassification">Video Action Recognition via Cross-Modality Learning</a>
            </blockquote>
            <div class="col-sm-4">
                <a href="https://github.com/zhaojw1998/SoundtrackClassification" class="portfolio-box">
                    <img src="img/portfolio/3.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Github Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>

            <div class="col-sm-8">
                <ul class="list-inline">
                    <ul class='text-justify'>
                        <li><p>Video Action Recognition is a popular computer vision task usually conducted exclusively on the image stream of the video. This project aims to explore the soundtrack and fuse the visual and auditory information for better recognition accuracy.</p></li>
                        <li><p>By training classification model with mel-spectrogram inputs, and through late fusion with the vision-based model prediction, we acquire a gain of 1.5% in recognition accuracy.
                        <li><p><a href="https://github.com/zhaojw1998/SoundtrackClassification" class="text-white bg-dark">Github Page</a></p></li>
                    </ul>
                </ul>     
            </div>
        </div>

        <p></p>

        <div class="row">
            <blockquote class="h4 text-left">
                <a href="https://github.com/zhaojw1998/Sound-Frequency-Analysis">Sound Frequency Analysis for Tightness Detection</a>
            </blockquote>
            <div class="col-sm-4">
                <a href="https://github.com/zhaojw1998/Sound-Frequency-Analysis" class="portfolio-box">
                    <img src="img/portfolio/7.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Github Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>

            <div class="col-sm-8">
                <ul class="list-inline">
                    <ul class='text-justify'>
                        <li><p>For the large-scale electricity generator set, tightness of the slot wedge (part of the generator) is critical to its functioning, but hard to directly detect. We propose a method through tapping at the wedge wall and analyzing the sound frequency to predict tightness.</p></li>
                        <li><p>By feeding the MFCC and log-energy of the waveform to a KNN classifier we gained an detection accuracy of 99.45% on the test set.
                            <li><p><a href="https://github.com/zhaojw1998/Sound-Frequency-Analysis" class="text-white bg-dark">Github Page</a></p></li>
                    </ul>
                </ul>     
            </div>
        </div>
        <p></p>
    </div>
    
</section>

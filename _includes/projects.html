<section class="bg-dark no-padding" id="Projects">
    <div class="container no-gutter">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">Selected Projects</h2>
                <hr class="primary">
            </div>
        </div>
        <div class="row ">
            <blockquote class="h4 text-left">
                <a href="project-auto_arrangement">Algorithm Arrangement via Search and Style Transfer</a>
            </blockquote>
            <div class="col-sm-4">
                <a href="project-auto_arrangement" class="portfolio-box">
                    <img src="img/portfolio/1.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Demo Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-sm-8">                
                <ul class="list-inline" >
                    <ul class='text-justify'>
                        <li><p> For a piece of melody query <em>Q<sub>M</sub></em>, we search for ideal target melody <em>T<sub>M</sub></em> through both latent-represented and rule-based criterion, and conduct arrangement by assigning <em>T<sub>M</sub></em>'s accompaniment track <em>T<sub>A</sub></em> to <em>Q<sub>M</sub></em>. </p></li>

                        <li><p>This project is a joint application of <a href="https://arxiv.org/abs/1906.03626" class="text-white bg-dark"><em>EC<sup>2</sup>-VAE</em></a>, <a href="https://arxiv.org/abs/2008.07122" class="text-white bg-dark"><em>Poly-Disentanglement-VAE</em></a>, and <a href="https://arxiv.org/abs/2008.07142" class="text-white bg-dark"><em>POP-909 Dataset</em></a>, three ISMIR works from <a href="http://www.musicxlab.com/#/" class="text-white bg-dark">Music X Lab</a>, NYU Shanghai.</p></li>

                    </ul>
                </ul>
            </div>
        </div>

        <p></p>
        
        <div class="row">
            <blockquote class="h4 text-left">
                <a href="project-dancing_robot">Real-Time Music-Driven Dance Generation for Humanoid Robot</a>
            </blockquote>
            <div class="col-sm-4">
                <a href="project-dancing_robot" class="portfolio-box">
                    <img src="img/portfolio/2.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Demo Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-sm-8">                
                <ul class="list-inline">
                    <ul class='text-justify'>
                        <li><p>This project aims to execute robot dance without hard coding, and we propose to generate improvisational dance sequence with Unit Selection Methodology based on two criterion: <em>semantic relevance of dance units</em>, and <em>unit transition stability of the robot</em></p></li>

                        <li><p>To render real-time performance, we apply <em>Madmom</em> for beat tracking and synchronize our dance to the music through a feedback loop with <em>PID</em> control</p></li>
                    </ul>
                </ul>
            </div>
        </div>

        <p></p>

        <div class="row">
            <blockquote class="h4 text-left">
                <a href="https://github.com/zhaojw1998/SoundtrackClassification">Video Action Recognition via Visual and Auditory Clues</a>
            </blockquote>
            <div class="col-sm-4">
                <a href="https://github.com/zhaojw1998/SoundtrackClassification" class="portfolio-box">
                    <img src="img/portfolio/3.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Github Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>

            <div class="col-sm-8">
                <ul class="list-inline">
                    <ul class='text-justify'>
                        <li><p>Video Action Recognition is a popular computer vision task usually conducted exclusively on the image stream of the video. This project aims to explore the soundtrack and fuse the visual and auditory information for better recognition accuracy</p></li>
                        <li><p>To reveal the inter-dependency between visual scene and sound in the video, we apply <em>scaled dot-product attention</em> as an early fusion methodology.
                    </ul>
                </ul>     
            </div>
        </div>

        <p></p>

        <div class="row">
            <blockquote class="h4 text-left">
                <a href="https://github.com/zhaojw1998/Sound-Frequency-Analysis">Sound Frequency Analysis for Tightness Detection</a>
            </blockquote>
            <div class="col-sm-4">
                <a href="https://github.com/zhaojw1998/Sound-Frequency-Analysis" class="portfolio-box">
                    <img src="img/portfolio/7.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Github Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>

            <div class="col-sm-8">
                <ul class="list-inline">
                    <ul class='text-justify'>
                        <li><p>For the large-scale electricity generator set, tightness of the <em>slot wedge</em> (part of the generator) is critical to its functioning, but hard to directly detect. Hence we propose a method through tapping at the wedge wall and analyzing the sound frequency.</p></li>
                        <li><p>By feeding the <em>MFCC</em> and <em>log-energy</em> of the waveform to a <em>KNN</em> classifier we gained an detection accuracy of 99.45% on the test set.
                    </ul>
                </ul>     
            </div>
        </div>
        <p></p>
    </div>
    
</section>
